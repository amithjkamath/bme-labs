{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubern-mia/bme-labs/blob/main/DLexample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c4qSOQimrSM"
      },
      "source": [
        "# Deep Learning Example\n",
        "\n",
        "This example gives a brief example on how to do a simple classification from MRI data. It takes images from the IXI dataset, where we have information about the sex of the subject alongside an MRI. We will try to directly infer the sex of a given patient from MRI. It is heavily inspired by this MONAI example: https://colab.research.google.com/github/Project-MONAI/MONAIBootcamp2021/blob/master/day1/3.%20End-To-End%20Workflow%20with%20MONAI.ipynb \n",
        "MONAI is a framework built on PyTorch specifically for deep learning with medical imaging data.\n",
        "\n",
        "To make the data easier to handle, the data is resized to a volume of 32x32x32 voxels. Preprocessing further included cropping to the foreground and z-score normalization.\n",
        "\n",
        "## Enabling GPU Support\n",
        "\n",
        "To use GPU resources through Colab, change the runtime to GPU:\n",
        "\n",
        "    From the \"Runtime\" menu select \"Change Runtime Type\"\n",
        "    Choose \"GPU\" from the drop-down menu\n",
        "    Click \"SAVE\"\n",
        "\n",
        "This will reset the notebook and probably ask you if you are a robot (these instructions assume you are not). Running\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "in a cell will verify this has worked and show you what kind of hardware you have access to.\n",
        "\n",
        "\n",
        "First, we download the data and install some required packages. We will use MONAI and PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHv7DXlGqIYO",
        "outputId": "e16cdf18-d3c2-459c-c224-bc7808298ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-10 16:05:37--  https://www.dropbox.com/s/c15o4fi5qjakeym/ixi_t1_fgcrop_32.h5\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/c15o4fi5qjakeym/ixi_t1_fgcrop_32.h5 [following]\n",
            "--2022-03-10 16:05:37--  https://www.dropbox.com/s/raw/c15o4fi5qjakeym/ixi_t1_fgcrop_32.h5\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucea51d7de7c7e5078ecfed1d11d.dl.dropboxusercontent.com/cd/0/inline/BhMQw9c-epexnyLXFzUW2PXlDwwG5GOsXmcd_6xe_ZIQ-jCBWrwtKFZ4S9702Vgv1uy6Nwb7CPs2RvWXbHNfDSi1NR_eeXJUJcBqqlHAzjVKJEfRnqB_YkoTmN-tpwHGHVmLTHYQ1zWBU4z_PFuopFippdEiVv8XbgTGdyYActiIBQ/file# [following]\n",
            "--2022-03-10 16:05:37--  https://ucea51d7de7c7e5078ecfed1d11d.dl.dropboxusercontent.com/cd/0/inline/BhMQw9c-epexnyLXFzUW2PXlDwwG5GOsXmcd_6xe_ZIQ-jCBWrwtKFZ4S9702Vgv1uy6Nwb7CPs2RvWXbHNfDSi1NR_eeXJUJcBqqlHAzjVKJEfRnqB_YkoTmN-tpwHGHVmLTHYQ1zWBU4z_PFuopFippdEiVv8XbgTGdyYActiIBQ/file\n",
            "Resolving ucea51d7de7c7e5078ecfed1d11d.dl.dropboxusercontent.com (ucea51d7de7c7e5078ecfed1d11d.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to ucea51d7de7c7e5078ecfed1d11d.dl.dropboxusercontent.com (ucea51d7de7c7e5078ecfed1d11d.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BhMbtOSIMohmsOC4EYrDgh56CSXnBB-pW54_rTAFk3s5fwv71InrLUCgbG212Q0VUmo6XupceZjGjc51RmIQSjZ97b9OcBO5zRKQyJAdgtTJdfooqkqUpuBnebVAq-UxlVV-wyozVVq95ljslpYQzI4GaYMldIoGNzvMKxtgXl20YEHahuXY9kluQSZKsMZoHGaL5tv05KtO_dl7MmytrE1rGkzgHRljgt6Ca9TXglamMXP7J3kobmGyugrF_aYZhEY_EjZrUx_5E62EeOwIsPwsXJ77at_XquVMjDwjZ5VRVfB8yzg4eHNuhrGWm_h36By4h_fppZCBirprhIHdG76nI6DXmKuZ55nFztRVa2-ymBZmGd_LjIHNkT0g7lKY8NjOu2WnqYOhU2VCc5-fwrUpDm8dUvHl2cc6JbH_VKEZbw/file [following]\n",
            "--2022-03-10 16:05:38--  https://ucea51d7de7c7e5078ecfed1d11d.dl.dropboxusercontent.com/cd/0/inline2/BhMbtOSIMohmsOC4EYrDgh56CSXnBB-pW54_rTAFk3s5fwv71InrLUCgbG212Q0VUmo6XupceZjGjc51RmIQSjZ97b9OcBO5zRKQyJAdgtTJdfooqkqUpuBnebVAq-UxlVV-wyozVVq95ljslpYQzI4GaYMldIoGNzvMKxtgXl20YEHahuXY9kluQSZKsMZoHGaL5tv05KtO_dl7MmytrE1rGkzgHRljgt6Ca9TXglamMXP7J3kobmGyugrF_aYZhEY_EjZrUx_5E62EeOwIsPwsXJ77at_XquVMjDwjZ5VRVfB8yzg4eHNuhrGWm_h36By4h_fppZCBirprhIHdG76nI6DXmKuZ55nFztRVa2-ymBZmGd_LjIHNkT0g7lKY8NjOu2WnqYOhU2VCc5-fwrUpDm8dUvHl2cc6JbH_VKEZbw/file\n",
            "Reusing existing connection to ucea51d7de7c7e5078ecfed1d11d.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 74730676 (71M) [application/octet-stream]\n",
            "Saving to: ‘ixi_t1_fgcrop_32.h5.1’\n",
            "\n",
            "ixi_t1_fgcrop_32.h5 100%[===================>]  71.27M  43.0MB/s    in 1.7s    \n",
            "\n",
            "2022-03-10 16:05:41 (43.0 MB/s) - ‘ixi_t1_fgcrop_32.h5.1’ saved [74730676/74730676]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.dropbox.com/s/c15o4fi5qjakeym/ixi_t1_fgcrop_32.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JAuGIcpo4vOh"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \"monai[ignite, nibabel, torchvision, tqdm]==0.6.0\"\n",
        "!pip install -qU \"pymia\"\n",
        "!pip install -qU \"torchio\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pJkwNwYrT7x",
        "outputId": "a0c341fa-9e54-4fce-81ea-803c08918801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 10 16:06:00 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roeBS_RPrN7U"
      },
      "source": [
        "## Data Loading\n",
        "The data we use comes in the hdf5 format. While we could also just load the individual images, this is more efficient. Data handling often is a bottleneck in the pipeline.\n",
        "\n",
        "To handle this dataset, we need some auxiliaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MHycObQkoqIx"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class H5Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, h5_path, transform=None):\n",
        "        super(H5Dataset, self).__init__()\n",
        "        h5_file = h5py.File(h5_path, 'r')\n",
        "        self.images = h5_file['data']['images']\n",
        "        self.labels = h5_file['data']['gt']\n",
        "        self.name = h5_file['meta']['subjects']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index_raw = int(index)\n",
        "        index = str(index).zfill(3)\n",
        "        img = torch.from_numpy(self.images[index][:]).float()\n",
        "        label = torch.from_numpy(self.labels[index][:]).to(torch.int64) + 1\n",
        "        subjects = self.name[index_raw][:].decode('utf-8')\n",
        "        return {\"images\": img,\n",
        "                \"labels\": label,\n",
        "                \"subjects\": subjects}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdM30hARpbDh"
      },
      "source": [
        "## The neural network\n",
        "\n",
        "Current state of the art networks probably have too many learnable parameters for this rather simple task. So we define a simpler network for this classification. It consists of "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "48PTXS5cpw2o"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.InstanceNorm3d(out_channels),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv3d(out_channels, out_channels, 3, 1, 1),\n",
        "            nn.InstanceNorm3d(out_channels),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.ones_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.ones_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.avg_pool3d(x, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, n_classes, n_input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv3d(in_channels=n_input_channels, out_channels=32, kernel_size=7, padding=3, stride=2),\n",
        "            nn.InstanceNorm3d(32),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        self.featextractor = nn.Sequential(\n",
        "            ConvBlock(in_channels=32, out_channels=64),\n",
        "            ConvBlock(in_channels=64, out_channels=128),\n",
        "            ConvBlock(in_channels=128, out_channels=256),\n",
        "            nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        )\n",
        "\n",
        "        self.drop_layer = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.fc = nn.Linear(256, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.featextractor(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.drop_layer(x)\n",
        "        out = self.fc(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pyHm3cmTGZxQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLKkp9VvtI4P"
      },
      "source": [
        "Next, we load the dataset and randomly create a train/test split. Please note, that we would create train/validation/test splits for a real application, but omit this for the [sake of simplicity](https://xkcd.com/2587/). We reserve 25% of the data for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-QhFtv8mrpGW"
      },
      "outputs": [],
      "source": [
        "dataset = H5Dataset(\"/content/ixi_t1_fgcrop_32.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dEM9J_3kCv33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee8df64-a909-4edb-cb0f-2478311890c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "566\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset.images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2Bm0XteXr36d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df025e6-d1e9-48d1-ebba-a9a5e2f19462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['000', '001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '086', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "subjectlist = np.arange(len(dataset.images))\n",
        "# (dataset.images.keys())\n",
        "print(list(dataset.images.keys()))\n",
        "\n",
        "# Reserve 20% of the data for testing\n",
        "subj_train, subj_test = train_test_split(subjectlist, test_size=0.25, \n",
        "                                         shuffle=True, random_state=42)\n",
        "\n",
        "trainingset = torch.utils.data.Subset(dataset, subj_train)\n",
        "testset = torch.utils.data.Subset(dataset, subj_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vi9lqAo2qEq"
      },
      "source": [
        "## Data loading and augmentations\n",
        "The very high number of parameters in common neural networks can lead to overfitting. To make this harder, we perturb the training data. This is called \"data augmentation\". In this example, we randomly flip the image, transform it with an affine matrix, add a bias field, noise and blurring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "83rFB8aZ242R"
      },
      "outputs": [],
      "source": [
        "import torchio as tio\n",
        "\n",
        "import pymia.data.transformation as tfm\n",
        "import pymia.data.definition as defs\n",
        "\n",
        "\n",
        "class TorchIOTransform(tfm.Transform):\n",
        "    \"\"\"Example wrapper for `TorchIO <https://github.com/fepegar/torchio>`_ transformations.\"\"\"\n",
        "\n",
        "    def __init__(self, transforms: list, entries=(defs.KEY_IMAGES, defs.KEY_LABELS)) -> None:\n",
        "        super().__init__()\n",
        "        self.transforms = transforms\n",
        "        self.entries = entries\n",
        "\n",
        "    def __call__(self, sample: dict) -> dict:\n",
        "        # unsqueeze samples to be 4-D tensors, as required by TorchIO\n",
        "        for entry in self.entries:\n",
        "            if entry not in sample:\n",
        "                if tfm.raise_error_if_entry_not_extracted:\n",
        "                    raise ValueError(tfm.ENTRY_NOT_EXTRACTED_ERR_MSG.format(entry))\n",
        "                continue\n",
        "\n",
        "            np_entry = tfm.check_and_return(sample[entry], np.ndarray)\n",
        "            sample[entry] = np.expand_dims(np_entry, -1)\n",
        "\n",
        "        # apply TorchIO transforms\n",
        "        for t in self.transforms:\n",
        "            sample = t(sample)\n",
        "\n",
        "        # squeeze samples back to original format\n",
        "        for entry in self.entries:\n",
        "            np_entry = tfm.check_and_return(sample[entry].numpy(), np.ndarray)\n",
        "            sample[entry] = np_entry.squeeze(-1)\n",
        "\n",
        "        return sample\n",
        "\n",
        "transforms_augmentation = [TorchIOTransform(\n",
        "    [tio.RandomFlip(axes='LR', flip_probability=0.5), \n",
        "     tio.RandomFlip(axes='AP', flip_probability=0.5),\n",
        "     tio.RandomAffine(scales=(0.85, 1.15), degrees=10, \n",
        "                      isotropic=False, default_pad_value='otsu',\n",
        "                      image_interpolation='NEAREST'),\n",
        "     tio.RandomBiasField(),\n",
        "     tio.RandomNoise(),\n",
        "     tio.RandomBlur(),\n",
        "     ])]\n",
        "\n",
        "batchsize = 566\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainingset, batch_size=batchsize, \n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          pin_memory=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize, \n",
        "                                         shuffle=False, num_workers=2, \n",
        "                                         pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_FttArf4CmC"
      },
      "source": [
        "Let's look at the classification model we created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QUabv1Ll38fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8853f6-b27b-47d5-927c-5cffa0f00a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier(\n",
            "  (conv0): Sequential(\n",
            "    (0): Conv3d(1, 32, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3))\n",
            "    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (featextractor): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (2): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (2): PReLU(num_parameters=1)\n",
            "      )\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (2): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (2): PReLU(num_parameters=1)\n",
            "      )\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (2): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (2): PReLU(num_parameters=1)\n",
            "      )\n",
            "    )\n",
            "    (3): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
            "  )\n",
            "  (drop_layer): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (conv0): Sequential(\n",
              "    (0): Conv3d(1, 32, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3))\n",
              "    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (2): PReLU(num_parameters=1)\n",
              "    (3): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (featextractor): Sequential(\n",
              "    (0): ConvBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        (2): PReLU(num_parameters=1)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        (2): PReLU(num_parameters=1)\n",
              "      )\n",
              "    )\n",
              "    (1): ConvBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        (2): PReLU(num_parameters=1)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        (2): PReLU(num_parameters=1)\n",
              "      )\n",
              "    )\n",
              "    (2): ConvBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        (2): PReLU(num_parameters=1)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        (2): PReLU(num_parameters=1)\n",
              "      )\n",
              "    )\n",
              "    (3): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
              "  )\n",
              "  (drop_layer): Dropout(p=0.2, inplace=False)\n",
              "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "assert torch.cuda.is_available(), \"GPU not available\"\n",
        "device = torch.device(\"cuda:0\")\n",
        "model = Classifier(n_classes=2, n_input_channels=1)\n",
        "print(model)\n",
        "# summary(model, (1, 32, 32, 32))\n",
        "model.to(device)\n",
        "# model = Classifier(n_classes=2, n_input_channels=1).to(device).float()\n",
        "\n",
        "# summary(model, (1, 32, 32, 32))\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lxsVppSqRLr"
      },
      "source": [
        "## The loss and optimizer\n",
        "\n",
        "Next, we need a loss fuction and an optimizer. We will use the cross-entropy loss and the \"Adam\" optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NzlGlbrX6EWy"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-4\n",
        "\n",
        "classificationloss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYGCPB3v6PQJ"
      },
      "source": [
        "## Training\n",
        "We have everything set up, from the data to the network, so we can finally train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_0u_6Pb6dQR",
        "outputId": "5b0b33cd-3570-4424-dd65-1cda29d99797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "epoch 1/200\n",
            "1/1, training_loss: 0.6932\n",
            "epoch 1 average loss: 0.3466\n",
            "Accuracy: 0.5212264150943396\n",
            "----------\n",
            "epoch 2/200\n",
            "1/1, training_loss: 0.6843\n",
            "epoch 2 average loss: 0.3421\n",
            "Accuracy: 0.5518867924528302\n",
            "----------\n",
            "epoch 3/200\n",
            "1/1, training_loss: 0.6718\n",
            "epoch 3 average loss: 0.3359\n",
            "Accuracy: 0.6132075471698113\n",
            "----------\n",
            "epoch 4/200\n",
            "1/1, training_loss: 0.6583\n",
            "epoch 4 average loss: 0.3291\n",
            "Accuracy: 0.6839622641509434\n",
            "----------\n",
            "epoch 5/200\n",
            "1/1, training_loss: 0.6501\n",
            "epoch 5 average loss: 0.3250\n",
            "Accuracy: 0.6816037735849056\n",
            "----------\n",
            "epoch 6/200\n",
            "1/1, training_loss: 0.6408\n",
            "epoch 6 average loss: 0.3204\n",
            "Accuracy: 0.6910377358490566\n",
            "----------\n",
            "epoch 7/200\n",
            "1/1, training_loss: 0.6229\n",
            "epoch 7 average loss: 0.3115\n",
            "Accuracy: 0.7712264150943396\n",
            "----------\n",
            "epoch 8/200\n",
            "1/1, training_loss: 0.6056\n",
            "epoch 8 average loss: 0.3028\n",
            "Accuracy: 0.8042452830188679\n",
            "----------\n",
            "epoch 9/200\n",
            "1/1, training_loss: 0.5845\n",
            "epoch 9 average loss: 0.2922\n",
            "Accuracy: 0.8325471698113207\n",
            "----------\n",
            "epoch 10/200\n",
            "1/1, training_loss: 0.5750\n",
            "epoch 10 average loss: 0.2875\n",
            "Accuracy: 0.8466981132075472\n",
            "----------\n",
            "epoch 11/200\n",
            "1/1, training_loss: 0.5790\n",
            "epoch 11 average loss: 0.2895\n",
            "Accuracy: 0.7735849056603774\n",
            "----------\n",
            "epoch 12/200\n",
            "1/1, training_loss: 0.5514\n",
            "epoch 12 average loss: 0.2757\n",
            "Accuracy: 0.8797169811320755\n",
            "----------\n",
            "epoch 13/200\n",
            "1/1, training_loss: 0.5446\n",
            "epoch 13 average loss: 0.2723\n",
            "Accuracy: 0.8773584905660378\n",
            "----------\n",
            "epoch 14/200\n",
            "1/1, training_loss: 0.5381\n",
            "epoch 14 average loss: 0.2691\n",
            "Accuracy: 0.8537735849056604\n",
            "----------\n",
            "epoch 15/200\n",
            "1/1, training_loss: 0.5233\n",
            "epoch 15 average loss: 0.2616\n",
            "Accuracy: 0.8632075471698113\n",
            "----------\n",
            "epoch 16/200\n",
            "1/1, training_loss: 0.5155\n",
            "epoch 16 average loss: 0.2578\n",
            "Accuracy: 0.8773584905660378\n",
            "----------\n",
            "epoch 17/200\n",
            "1/1, training_loss: 0.5039\n",
            "epoch 17 average loss: 0.2520\n",
            "Accuracy: 0.875\n",
            "----------\n",
            "epoch 18/200\n",
            "1/1, training_loss: 0.4999\n",
            "epoch 18 average loss: 0.2499\n",
            "Accuracy: 0.875\n",
            "----------\n",
            "epoch 19/200\n",
            "1/1, training_loss: 0.4865\n",
            "epoch 19 average loss: 0.2432\n",
            "Accuracy: 0.8962264150943396\n",
            "----------\n",
            "epoch 20/200\n",
            "1/1, training_loss: 0.4835\n",
            "epoch 20 average loss: 0.2418\n",
            "Accuracy: 0.9033018867924528\n",
            "----------\n",
            "epoch 21/200\n",
            "1/1, training_loss: 0.4701\n",
            "epoch 21 average loss: 0.2350\n",
            "Accuracy: 0.8985849056603774\n",
            "----------\n",
            "epoch 22/200\n",
            "1/1, training_loss: 0.4651\n",
            "epoch 22 average loss: 0.2325\n",
            "Accuracy: 0.9033018867924528\n",
            "----------\n",
            "epoch 23/200\n",
            "1/1, training_loss: 0.4655\n",
            "epoch 23 average loss: 0.2327\n",
            "Accuracy: 0.9127358490566038\n",
            "----------\n",
            "epoch 24/200\n",
            "1/1, training_loss: 0.4569\n",
            "epoch 24 average loss: 0.2284\n",
            "Accuracy: 0.9127358490566038\n",
            "----------\n",
            "epoch 25/200\n",
            "1/1, training_loss: 0.4492\n",
            "epoch 25 average loss: 0.2246\n",
            "Accuracy: 0.9127358490566038\n",
            "----------\n",
            "epoch 26/200\n",
            "1/1, training_loss: 0.4431\n",
            "epoch 26 average loss: 0.2215\n",
            "Accuracy: 0.9339622641509434\n",
            "----------\n",
            "epoch 27/200\n",
            "1/1, training_loss: 0.4402\n",
            "epoch 27 average loss: 0.2201\n",
            "Accuracy: 0.9268867924528302\n",
            "----------\n",
            "epoch 28/200\n",
            "1/1, training_loss: 0.4305\n",
            "epoch 28 average loss: 0.2152\n",
            "Accuracy: 0.9268867924528302\n",
            "----------\n",
            "epoch 29/200\n",
            "1/1, training_loss: 0.4231\n",
            "epoch 29 average loss: 0.2116\n",
            "Accuracy: 0.9457547169811321\n",
            "----------\n",
            "epoch 30/200\n",
            "1/1, training_loss: 0.4171\n",
            "epoch 30 average loss: 0.2086\n",
            "Accuracy: 0.9481132075471698\n",
            "----------\n",
            "epoch 31/200\n",
            "1/1, training_loss: 0.4140\n",
            "epoch 31 average loss: 0.2070\n",
            "Accuracy: 0.9410377358490566\n",
            "----------\n",
            "epoch 32/200\n",
            "1/1, training_loss: 0.4073\n",
            "epoch 32 average loss: 0.2037\n",
            "Accuracy: 0.9528301886792453\n",
            "----------\n",
            "epoch 33/200\n",
            "1/1, training_loss: 0.4008\n",
            "epoch 33 average loss: 0.2004\n",
            "Accuracy: 0.9504716981132075\n",
            "----------\n",
            "epoch 34/200\n",
            "1/1, training_loss: 0.3988\n",
            "epoch 34 average loss: 0.1994\n",
            "Accuracy: 0.9504716981132075\n",
            "----------\n",
            "epoch 35/200\n",
            "1/1, training_loss: 0.3955\n",
            "epoch 35 average loss: 0.1978\n",
            "Accuracy: 0.9528301886792453\n",
            "----------\n",
            "epoch 36/200\n",
            "1/1, training_loss: 0.3919\n",
            "epoch 36 average loss: 0.1959\n",
            "Accuracy: 0.9504716981132075\n",
            "----------\n",
            "epoch 37/200\n",
            "1/1, training_loss: 0.3892\n",
            "epoch 37 average loss: 0.1946\n",
            "Accuracy: 0.9575471698113207\n",
            "----------\n",
            "epoch 38/200\n",
            "1/1, training_loss: 0.3787\n",
            "epoch 38 average loss: 0.1893\n",
            "Accuracy: 0.964622641509434\n",
            "----------\n",
            "epoch 39/200\n",
            "1/1, training_loss: 0.3791\n",
            "epoch 39 average loss: 0.1895\n",
            "Accuracy: 0.9575471698113207\n",
            "----------\n",
            "epoch 40/200\n",
            "1/1, training_loss: 0.3774\n",
            "epoch 40 average loss: 0.1887\n",
            "Accuracy: 0.9622641509433962\n",
            "----------\n",
            "epoch 41/200\n",
            "1/1, training_loss: 0.3703\n",
            "epoch 41 average loss: 0.1852\n",
            "Accuracy: 0.964622641509434\n",
            "----------\n",
            "epoch 42/200\n",
            "1/1, training_loss: 0.3660\n",
            "epoch 42 average loss: 0.1830\n",
            "Accuracy: 0.9622641509433962\n",
            "----------\n",
            "epoch 43/200\n",
            "1/1, training_loss: 0.3640\n",
            "epoch 43 average loss: 0.1820\n",
            "Accuracy: 0.964622641509434\n",
            "----------\n",
            "epoch 44/200\n",
            "1/1, training_loss: 0.3653\n",
            "epoch 44 average loss: 0.1827\n",
            "Accuracy: 0.9599056603773585\n",
            "----------\n",
            "epoch 45/200\n",
            "1/1, training_loss: 0.3634\n",
            "epoch 45 average loss: 0.1817\n",
            "Accuracy: 0.9599056603773585\n",
            "----------\n",
            "epoch 46/200\n",
            "1/1, training_loss: 0.3543\n",
            "epoch 46 average loss: 0.1771\n",
            "Accuracy: 0.9693396226415094\n",
            "----------\n",
            "epoch 47/200\n",
            "1/1, training_loss: 0.3518\n",
            "epoch 47 average loss: 0.1759\n",
            "Accuracy: 0.9693396226415094\n",
            "----------\n",
            "epoch 48/200\n",
            "1/1, training_loss: 0.3442\n",
            "epoch 48 average loss: 0.1721\n",
            "Accuracy: 0.9669811320754716\n",
            "----------\n",
            "epoch 49/200\n",
            "1/1, training_loss: 0.3425\n",
            "epoch 49 average loss: 0.1713\n",
            "Accuracy: 0.9716981132075472\n",
            "----------\n",
            "epoch 50/200\n",
            "1/1, training_loss: 0.3405\n",
            "epoch 50 average loss: 0.1703\n",
            "Accuracy: 0.9716981132075472\n",
            "----------\n",
            "epoch 51/200\n",
            "1/1, training_loss: 0.3338\n",
            "epoch 51 average loss: 0.1669\n",
            "Accuracy: 0.9740566037735849\n",
            "----------\n",
            "epoch 52/200\n",
            "1/1, training_loss: 0.3313\n",
            "epoch 52 average loss: 0.1657\n",
            "Accuracy: 0.9716981132075472\n",
            "----------\n",
            "epoch 53/200\n",
            "1/1, training_loss: 0.3289\n",
            "epoch 53 average loss: 0.1644\n",
            "Accuracy: 0.9764150943396226\n",
            "----------\n",
            "epoch 54/200\n",
            "1/1, training_loss: 0.3240\n",
            "epoch 54 average loss: 0.1620\n",
            "Accuracy: 0.9787735849056604\n",
            "----------\n",
            "epoch 55/200\n",
            "1/1, training_loss: 0.3279\n",
            "epoch 55 average loss: 0.1639\n",
            "Accuracy: 0.9811320754716981\n",
            "----------\n",
            "epoch 56/200\n",
            "1/1, training_loss: 0.3213\n",
            "epoch 56 average loss: 0.1607\n",
            "Accuracy: 0.9787735849056604\n",
            "----------\n",
            "epoch 57/200\n",
            "1/1, training_loss: 0.3206\n",
            "epoch 57 average loss: 0.1603\n",
            "Accuracy: 0.9764150943396226\n",
            "----------\n",
            "epoch 58/200\n",
            "1/1, training_loss: 0.3171\n",
            "epoch 58 average loss: 0.1585\n",
            "Accuracy: 0.9811320754716981\n",
            "----------\n",
            "epoch 59/200\n",
            "1/1, training_loss: 0.3109\n",
            "epoch 59 average loss: 0.1555\n",
            "Accuracy: 0.9787735849056604\n",
            "----------\n",
            "epoch 60/200\n",
            "1/1, training_loss: 0.3060\n",
            "epoch 60 average loss: 0.1530\n",
            "Accuracy: 0.9858490566037735\n",
            "----------\n",
            "epoch 61/200\n",
            "1/1, training_loss: 0.3037\n",
            "epoch 61 average loss: 0.1518\n",
            "Accuracy: 0.9834905660377359\n",
            "----------\n",
            "epoch 62/200\n",
            "1/1, training_loss: 0.3011\n",
            "epoch 62 average loss: 0.1506\n",
            "Accuracy: 0.9882075471698113\n",
            "----------\n",
            "epoch 63/200\n",
            "1/1, training_loss: 0.3047\n",
            "epoch 63 average loss: 0.1523\n",
            "Accuracy: 0.9787735849056604\n",
            "----------\n",
            "epoch 64/200\n",
            "1/1, training_loss: 0.3015\n",
            "epoch 64 average loss: 0.1507\n",
            "Accuracy: 0.9858490566037735\n",
            "----------\n",
            "epoch 65/200\n",
            "1/1, training_loss: 0.2930\n",
            "epoch 65 average loss: 0.1465\n",
            "Accuracy: 0.9905660377358491\n",
            "----------\n",
            "epoch 66/200\n",
            "1/1, training_loss: 0.2915\n",
            "epoch 66 average loss: 0.1458\n",
            "Accuracy: 0.9905660377358491\n",
            "----------\n",
            "epoch 67/200\n",
            "1/1, training_loss: 0.2926\n",
            "epoch 67 average loss: 0.1463\n",
            "Accuracy: 0.9929245283018868\n",
            "----------\n",
            "epoch 68/200\n",
            "1/1, training_loss: 0.2890\n",
            "epoch 68 average loss: 0.1445\n",
            "Accuracy: 0.9882075471698113\n",
            "----------\n",
            "epoch 69/200\n",
            "1/1, training_loss: 0.2829\n",
            "epoch 69 average loss: 0.1415\n",
            "Accuracy: 0.9882075471698113\n",
            "----------\n",
            "epoch 70/200\n",
            "1/1, training_loss: 0.2807\n",
            "epoch 70 average loss: 0.1404\n",
            "Accuracy: 0.9905660377358491\n",
            "----------\n",
            "epoch 71/200\n",
            "1/1, training_loss: 0.2768\n",
            "epoch 71 average loss: 0.1384\n",
            "Accuracy: 0.9929245283018868\n",
            "----------\n",
            "epoch 72/200\n",
            "1/1, training_loss: 0.2769\n",
            "epoch 72 average loss: 0.1385\n",
            "Accuracy: 0.9952830188679245\n",
            "----------\n",
            "epoch 73/200\n",
            "1/1, training_loss: 0.2708\n",
            "epoch 73 average loss: 0.1354\n",
            "Accuracy: 0.9952830188679245\n",
            "----------\n",
            "epoch 74/200\n",
            "1/1, training_loss: 0.2679\n",
            "epoch 74 average loss: 0.1340\n",
            "Accuracy: 0.9952830188679245\n",
            "----------\n",
            "epoch 75/200\n",
            "1/1, training_loss: 0.2687\n",
            "epoch 75 average loss: 0.1344\n",
            "Accuracy: 0.9952830188679245\n",
            "----------\n",
            "epoch 76/200\n",
            "1/1, training_loss: 0.2673\n",
            "epoch 76 average loss: 0.1337\n",
            "Accuracy: 0.9929245283018868\n",
            "----------\n",
            "epoch 77/200\n",
            "1/1, training_loss: 0.2626\n",
            "epoch 77 average loss: 0.1313\n",
            "Accuracy: 0.9976415094339622\n",
            "----------\n",
            "epoch 78/200\n",
            "1/1, training_loss: 0.2623\n",
            "epoch 78 average loss: 0.1311\n",
            "Accuracy: 0.9952830188679245\n",
            "----------\n",
            "epoch 79/200\n",
            "1/1, training_loss: 0.2582\n",
            "epoch 79 average loss: 0.1291\n",
            "Accuracy: 0.9976415094339622\n",
            "----------\n",
            "epoch 80/200\n",
            "1/1, training_loss: 0.2592\n",
            "epoch 80 average loss: 0.1296\n",
            "Accuracy: 0.9976415094339622\n",
            "----------\n",
            "epoch 81/200\n",
            "1/1, training_loss: 0.2557\n",
            "epoch 81 average loss: 0.1279\n",
            "Accuracy: 0.9976415094339622\n",
            "----------\n",
            "epoch 82/200\n",
            "1/1, training_loss: 0.2539\n",
            "epoch 82 average loss: 0.1270\n",
            "Accuracy: 0.9976415094339622\n",
            "----------\n",
            "epoch 83/200\n",
            "1/1, training_loss: 0.2505\n",
            "epoch 83 average loss: 0.1252\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 84/200\n",
            "1/1, training_loss: 0.2499\n",
            "epoch 84 average loss: 0.1249\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 85/200\n",
            "1/1, training_loss: 0.2480\n",
            "epoch 85 average loss: 0.1240\n",
            "Accuracy: 0.9976415094339622\n",
            "----------\n",
            "epoch 86/200\n",
            "1/1, training_loss: 0.2446\n",
            "epoch 86 average loss: 0.1223\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 87/200\n",
            "1/1, training_loss: 0.2438\n",
            "epoch 87 average loss: 0.1219\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 88/200\n",
            "1/1, training_loss: 0.2382\n",
            "epoch 88 average loss: 0.1191\n",
            "Accuracy: 0.9976415094339622\n",
            "----------\n",
            "epoch 89/200\n",
            "1/1, training_loss: 0.2408\n",
            "epoch 89 average loss: 0.1204\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 90/200\n",
            "1/1, training_loss: 0.2374\n",
            "epoch 90 average loss: 0.1187\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 91/200\n",
            "1/1, training_loss: 0.2398\n",
            "epoch 91 average loss: 0.1199\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 92/200\n",
            "1/1, training_loss: 0.2353\n",
            "epoch 92 average loss: 0.1177\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 93/200\n",
            "1/1, training_loss: 0.2335\n",
            "epoch 93 average loss: 0.1168\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 94/200\n",
            "1/1, training_loss: 0.2317\n",
            "epoch 94 average loss: 0.1159\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 95/200\n",
            "1/1, training_loss: 0.2309\n",
            "epoch 95 average loss: 0.1155\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 96/200\n",
            "1/1, training_loss: 0.2279\n",
            "epoch 96 average loss: 0.1139\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 97/200\n",
            "1/1, training_loss: 0.2280\n",
            "epoch 97 average loss: 0.1140\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 98/200\n",
            "1/1, training_loss: 0.2251\n",
            "epoch 98 average loss: 0.1125\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 99/200\n",
            "1/1, training_loss: 0.2254\n",
            "epoch 99 average loss: 0.1127\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 100/200\n",
            "1/1, training_loss: 0.2250\n",
            "epoch 100 average loss: 0.1125\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 101/200\n",
            "1/1, training_loss: 0.2252\n",
            "epoch 101 average loss: 0.1126\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 102/200\n",
            "1/1, training_loss: 0.2194\n",
            "epoch 102 average loss: 0.1097\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 103/200\n",
            "1/1, training_loss: 0.2180\n",
            "epoch 103 average loss: 0.1090\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 104/200\n",
            "1/1, training_loss: 0.2149\n",
            "epoch 104 average loss: 0.1075\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 105/200\n",
            "1/1, training_loss: 0.2179\n",
            "epoch 105 average loss: 0.1089\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 106/200\n",
            "1/1, training_loss: 0.2141\n",
            "epoch 106 average loss: 0.1071\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 107/200\n",
            "1/1, training_loss: 0.2139\n",
            "epoch 107 average loss: 0.1069\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 108/200\n",
            "1/1, training_loss: 0.2130\n",
            "epoch 108 average loss: 0.1065\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 109/200\n",
            "1/1, training_loss: 0.2094\n",
            "epoch 109 average loss: 0.1047\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 110/200\n",
            "1/1, training_loss: 0.2069\n",
            "epoch 110 average loss: 0.1034\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 111/200\n",
            "1/1, training_loss: 0.2093\n",
            "epoch 111 average loss: 0.1047\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 112/200\n",
            "1/1, training_loss: 0.2065\n",
            "epoch 112 average loss: 0.1032\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 113/200\n",
            "1/1, training_loss: 0.2070\n",
            "epoch 113 average loss: 0.1035\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 114/200\n",
            "1/1, training_loss: 0.2067\n",
            "epoch 114 average loss: 0.1033\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 115/200\n",
            "1/1, training_loss: 0.2033\n",
            "epoch 115 average loss: 0.1016\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 116/200\n",
            "1/1, training_loss: 0.2054\n",
            "epoch 116 average loss: 0.1027\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 117/200\n",
            "1/1, training_loss: 0.2046\n",
            "epoch 117 average loss: 0.1023\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 118/200\n",
            "1/1, training_loss: 0.2019\n",
            "epoch 118 average loss: 0.1010\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 119/200\n",
            "1/1, training_loss: 0.1994\n",
            "epoch 119 average loss: 0.0997\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 120/200\n",
            "1/1, training_loss: 0.1978\n",
            "epoch 120 average loss: 0.0989\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 121/200\n",
            "1/1, training_loss: 0.2011\n",
            "epoch 121 average loss: 0.1005\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 122/200\n",
            "1/1, training_loss: 0.1939\n",
            "epoch 122 average loss: 0.0970\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 123/200\n",
            "1/1, training_loss: 0.1960\n",
            "epoch 123 average loss: 0.0980\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 124/200\n",
            "1/1, training_loss: 0.1960\n",
            "epoch 124 average loss: 0.0980\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 125/200\n",
            "1/1, training_loss: 0.1938\n",
            "epoch 125 average loss: 0.0969\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 126/200\n",
            "1/1, training_loss: 0.1929\n",
            "epoch 126 average loss: 0.0965\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 127/200\n",
            "1/1, training_loss: 0.1934\n",
            "epoch 127 average loss: 0.0967\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 128/200\n",
            "1/1, training_loss: 0.1902\n",
            "epoch 128 average loss: 0.0951\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 129/200\n",
            "1/1, training_loss: 0.1901\n",
            "epoch 129 average loss: 0.0951\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 130/200\n",
            "1/1, training_loss: 0.1907\n",
            "epoch 130 average loss: 0.0953\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 131/200\n",
            "1/1, training_loss: 0.1871\n",
            "epoch 131 average loss: 0.0936\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 132/200\n",
            "1/1, training_loss: 0.1872\n",
            "epoch 132 average loss: 0.0936\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 133/200\n",
            "1/1, training_loss: 0.1865\n",
            "epoch 133 average loss: 0.0933\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 134/200\n",
            "1/1, training_loss: 0.1860\n",
            "epoch 134 average loss: 0.0930\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 135/200\n",
            "1/1, training_loss: 0.1849\n",
            "epoch 135 average loss: 0.0924\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 136/200\n",
            "1/1, training_loss: 0.1847\n",
            "epoch 136 average loss: 0.0923\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 137/200\n",
            "1/1, training_loss: 0.1840\n",
            "epoch 137 average loss: 0.0920\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 138/200\n",
            "1/1, training_loss: 0.1827\n",
            "epoch 138 average loss: 0.0914\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 139/200\n",
            "1/1, training_loss: 0.1821\n",
            "epoch 139 average loss: 0.0911\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 140/200\n",
            "1/1, training_loss: 0.1814\n",
            "epoch 140 average loss: 0.0907\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 141/200\n",
            "1/1, training_loss: 0.1783\n",
            "epoch 141 average loss: 0.0892\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 142/200\n",
            "1/1, training_loss: 0.1768\n",
            "epoch 142 average loss: 0.0884\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 143/200\n",
            "1/1, training_loss: 0.1769\n",
            "epoch 143 average loss: 0.0885\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 144/200\n",
            "1/1, training_loss: 0.1745\n",
            "epoch 144 average loss: 0.0872\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 145/200\n",
            "1/1, training_loss: 0.1738\n",
            "epoch 145 average loss: 0.0869\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 146/200\n",
            "1/1, training_loss: 0.1750\n",
            "epoch 146 average loss: 0.0875\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 147/200\n",
            "1/1, training_loss: 0.1750\n",
            "epoch 147 average loss: 0.0875\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 148/200\n",
            "1/1, training_loss: 0.1740\n",
            "epoch 148 average loss: 0.0870\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 149/200\n",
            "1/1, training_loss: 0.1742\n",
            "epoch 149 average loss: 0.0871\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 150/200\n",
            "1/1, training_loss: 0.1731\n",
            "epoch 150 average loss: 0.0865\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 151/200\n",
            "1/1, training_loss: 0.1716\n",
            "epoch 151 average loss: 0.0858\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 152/200\n",
            "1/1, training_loss: 0.1707\n",
            "epoch 152 average loss: 0.0853\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 153/200\n",
            "1/1, training_loss: 0.1693\n",
            "epoch 153 average loss: 0.0846\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 154/200\n",
            "1/1, training_loss: 0.1700\n",
            "epoch 154 average loss: 0.0850\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 155/200\n",
            "1/1, training_loss: 0.1667\n",
            "epoch 155 average loss: 0.0834\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 156/200\n",
            "1/1, training_loss: 0.1675\n",
            "epoch 156 average loss: 0.0838\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 157/200\n",
            "1/1, training_loss: 0.1686\n",
            "epoch 157 average loss: 0.0843\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 158/200\n",
            "1/1, training_loss: 0.1660\n",
            "epoch 158 average loss: 0.0830\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 159/200\n",
            "1/1, training_loss: 0.1646\n",
            "epoch 159 average loss: 0.0823\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 160/200\n",
            "1/1, training_loss: 0.1651\n",
            "epoch 160 average loss: 0.0825\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 161/200\n",
            "1/1, training_loss: 0.1642\n",
            "epoch 161 average loss: 0.0821\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 162/200\n",
            "1/1, training_loss: 0.1607\n",
            "epoch 162 average loss: 0.0804\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 163/200\n",
            "1/1, training_loss: 0.1645\n",
            "epoch 163 average loss: 0.0822\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 164/200\n",
            "1/1, training_loss: 0.1613\n",
            "epoch 164 average loss: 0.0807\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 165/200\n",
            "1/1, training_loss: 0.1620\n",
            "epoch 165 average loss: 0.0810\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 166/200\n",
            "1/1, training_loss: 0.1618\n",
            "epoch 166 average loss: 0.0809\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 167/200\n",
            "1/1, training_loss: 0.1603\n",
            "epoch 167 average loss: 0.0801\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 168/200\n",
            "1/1, training_loss: 0.1598\n",
            "epoch 168 average loss: 0.0799\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 169/200\n",
            "1/1, training_loss: 0.1574\n",
            "epoch 169 average loss: 0.0787\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 170/200\n",
            "1/1, training_loss: 0.1575\n",
            "epoch 170 average loss: 0.0788\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 171/200\n",
            "1/1, training_loss: 0.1558\n",
            "epoch 171 average loss: 0.0779\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 172/200\n",
            "1/1, training_loss: 0.1545\n",
            "epoch 172 average loss: 0.0772\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 173/200\n",
            "1/1, training_loss: 0.1548\n",
            "epoch 173 average loss: 0.0774\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 174/200\n",
            "1/1, training_loss: 0.1532\n",
            "epoch 174 average loss: 0.0766\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 175/200\n",
            "1/1, training_loss: 0.1550\n",
            "epoch 175 average loss: 0.0775\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 176/200\n",
            "1/1, training_loss: 0.1535\n",
            "epoch 176 average loss: 0.0767\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 177/200\n",
            "1/1, training_loss: 0.1530\n",
            "epoch 177 average loss: 0.0765\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 178/200\n",
            "1/1, training_loss: 0.1539\n",
            "epoch 178 average loss: 0.0770\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 179/200\n",
            "1/1, training_loss: 0.1494\n",
            "epoch 179 average loss: 0.0747\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 180/200\n",
            "1/1, training_loss: 0.1524\n",
            "epoch 180 average loss: 0.0762\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 181/200\n",
            "1/1, training_loss: 0.1496\n",
            "epoch 181 average loss: 0.0748\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 182/200\n",
            "1/1, training_loss: 0.1487\n",
            "epoch 182 average loss: 0.0744\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 183/200\n",
            "1/1, training_loss: 0.1511\n",
            "epoch 183 average loss: 0.0755\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 184/200\n",
            "1/1, training_loss: 0.1511\n",
            "epoch 184 average loss: 0.0756\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 185/200\n",
            "1/1, training_loss: 0.1511\n",
            "epoch 185 average loss: 0.0755\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 186/200\n",
            "1/1, training_loss: 0.1457\n",
            "epoch 186 average loss: 0.0728\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 187/200\n",
            "1/1, training_loss: 0.1467\n",
            "epoch 187 average loss: 0.0734\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 188/200\n",
            "1/1, training_loss: 0.1480\n",
            "epoch 188 average loss: 0.0740\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 189/200\n",
            "1/1, training_loss: 0.1446\n",
            "epoch 189 average loss: 0.0723\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 190/200\n",
            "1/1, training_loss: 0.1455\n",
            "epoch 190 average loss: 0.0728\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 191/200\n",
            "1/1, training_loss: 0.1447\n",
            "epoch 191 average loss: 0.0723\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 192/200\n",
            "1/1, training_loss: 0.1446\n",
            "epoch 192 average loss: 0.0723\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 193/200\n",
            "1/1, training_loss: 0.1438\n",
            "epoch 193 average loss: 0.0719\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 194/200\n",
            "1/1, training_loss: 0.1438\n",
            "epoch 194 average loss: 0.0719\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 195/200\n",
            "1/1, training_loss: 0.1446\n",
            "epoch 195 average loss: 0.0723\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 196/200\n",
            "1/1, training_loss: 0.1406\n",
            "epoch 196 average loss: 0.0703\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 197/200\n",
            "1/1, training_loss: 0.1397\n",
            "epoch 197 average loss: 0.0699\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 198/200\n",
            "1/1, training_loss: 0.1414\n",
            "epoch 198 average loss: 0.0707\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 199/200\n",
            "1/1, training_loss: 0.1395\n",
            "epoch 199 average loss: 0.0697\n",
            "Accuracy: 1.0\n",
            "----------\n",
            "epoch 200/200\n",
            "1/1, training_loss: 0.1415\n",
            "epoch 200 average loss: 0.0707\n",
            "Accuracy: 1.0\n",
            "Done :-)\n"
          ]
        }
      ],
      "source": [
        "from monai.metrics import ROCAUCMetric, ConfusionMatrixMetric\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, \\\n",
        "    roc_auc_score\n",
        "\n",
        "epoch_num = 200\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = list()\n",
        "metric_values = list()\n",
        "auc_metric = ROCAUCMetric()\n",
        "accuracy = list()\n",
        "train_loss = list()\n",
        "val_loss = list()\n",
        "train_accuracy = list()\n",
        "val_accuracy = list()\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_loss_val = 0\n",
        "    step = 1\n",
        "\n",
        "    steps_per_epoch = len(trainingset) // trainloader.batch_size\n",
        "    pred_values = list()\n",
        "    label_values = list()\n",
        "    pred_values_test = list()\n",
        "    label_values_test = list()\n",
        "\n",
        "    # put the network in train mode; this tells the network and its modules to\n",
        "    # enable training elements such as normalisation and dropout, where applicable\n",
        "    model.train()\n",
        "    for batch in trainloader:\n",
        "\n",
        "      inputs, labels = batch[\"images\"].swapaxes(1, -1).cuda(), batch[\"labels\"].squeeze(1).cuda()\n",
        "\n",
        "      # prepare the gradients for this step's back propagation\n",
        "      optimizer.zero_grad()\n",
        "        \n",
        "      # run the network forwards\n",
        "      outputs = model(inputs)\n",
        "        \n",
        "      # run the loss function on the outputs\n",
        "      loss = classificationloss(outputs, labels)\n",
        "        \n",
        "      # compute the gradients\n",
        "      loss.backward()\n",
        "        \n",
        "      # tell the optimizer to update the weights according to the gradients\n",
        "      # and its internal optimisation strategy\n",
        "      optimizer.step()\n",
        "\n",
        "      label_values += labels.cpu()\n",
        "      pred_values += outputs.argmax(dim=1).cpu()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      print(f\"{step}/{len(trainingset) // trainloader.batch_size + 1}, training_loss: {loss.item():.4f}\")\n",
        "      step += 1\n",
        "\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    train_accuracy.append(accuracy_score(label_values, pred_values))\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "    print(\"Accuracy: \" + str(accuracy_score(label_values, pred_values)))\n",
        "    \n",
        "        # switch off training features of the network for this pass\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    # 'with torch.no_grad()' switches off gradient calculation for the scope of its context\n",
        "    with torch.no_grad():\n",
        "        # create lists to which we will concatenate the the validation results\n",
        "\n",
        "\n",
        "        val_step = 0\n",
        "        # iterate over each batch of images and run them through the network in evaluation mode\n",
        "        for val_data in testloader:\n",
        "            val_images, val_labels = batch[\"images\"].swapaxes(1, -1).cuda(), batch[\"labels\"].squeeze(1).cuda()\n",
        "\n",
        "            # run the network\n",
        "            val_out = model(val_images)\n",
        "\n",
        "            test_loss = classificationloss(val_out, val_labels)\n",
        "            epoch_loss_val += test_loss.item()\n",
        "\n",
        "            pred_values_test += val_out.argmax(dim=1).cpu()\n",
        "            label_values_test += val_labels.cpu()\n",
        "            val_step += 1\n",
        "\n",
        "        val_accuracy.append(accuracy_score(label_values_test, pred_values_test))\n",
        "        epoch_loss_val /= val_step\n",
        "        val_loss.append(epoch_loss_val)           \n",
        "\n",
        "print(\"Done :-)\")\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can check how the loss and the accuracy evolves over the training process:"
      ],
      "metadata": {
        "id": "lw_U855XnPW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = np.arange(1, len(epoch_loss_values) +1)\n",
        "\n",
        "plt.figure(\"train/test\", (12, 6))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, epoch_loss_values)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.title(\"Test loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, val_loss)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.title(\"Training accuracy\")\n",
        "y = train_accuracy\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, train_accuracy)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.title(\"Test accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, val_accuracy)\n",
        "plt.suptitle(\"Sex classification IXI dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "pPRv0-ajA0EH",
        "outputId": "bc817e64-c6fe-4cba-f055-8dd8645207cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5448113207547169, 0.5542452830188679, 0.6179245283018868, 0.6745283018867925, 0.7759433962264151, 0.7900943396226415, 0.8372641509433962, 0.839622641509434, 0.8867924528301887, 0.7806603773584906, 0.8891509433962265, 0.8985849056603774, 0.8490566037735849, 0.8773584905660378, 0.8938679245283019, 0.8962264150943396, 0.8702830188679245, 0.9150943396226415, 0.9198113207547169, 0.8962264150943396, 0.910377358490566, 0.9245283018867925, 0.9221698113207547, 0.9127358490566038, 0.9339622641509434, 0.9316037735849056, 0.9268867924528302, 0.9386792452830188, 0.9481132075471698, 0.9386792452830188, 0.9504716981132075, 0.9504716981132075, 0.9457547169811321, 0.9551886792452831, 0.9575471698113207, 0.9575471698113207, 0.9575471698113207, 0.9622641509433962, 0.9622641509433962, 0.9622641509433962, 0.9622641509433962, 0.9622641509433962, 0.964622641509434, 0.9622641509433962, 0.9693396226415094, 0.9716981132075472, 0.9716981132075472, 0.9693396226415094, 0.9740566037735849, 0.9716981132075472, 0.9740566037735849, 0.9787735849056604, 0.9740566037735849, 0.9787735849056604, 0.9764150943396226, 0.9787735849056604, 0.9811320754716981, 0.9787735849056604, 0.9834905660377359, 0.9882075471698113, 0.9811320754716981, 0.9834905660377359, 0.9811320754716981, 0.9905660377358491, 0.9905660377358491, 0.9882075471698113, 0.9905660377358491, 0.9929245283018868, 0.9929245283018868, 0.9952830188679245, 0.9952830188679245, 0.9952830188679245, 0.9952830188679245, 0.9976415094339622, 0.9952830188679245, 0.9952830188679245, 0.9976415094339622, 0.9976415094339622, 0.9976415094339622, 0.9976415094339622, 0.9976415094339622, 0.9976415094339622, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DLexample.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP8bdX/Agdry/1Ek6b2/p+I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}